{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL 1: Install libraries and Cloudflare tunnel binary ====\n",
        "\n",
        "!pip install -q diffusers transformers accelerate safetensors \\\n",
        "               streamlit pillow\n",
        "\n",
        "# Download cloudflared binary for tunneling Streamlit\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "\n",
        "print(\"‚úÖ Installed: diffusers, transformers, accelerate, safetensors, streamlit, pillow, cloudflared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ZnF1yQhMs6",
        "outputId": "b3e7fde9-5a7b-413d-aedf-3a9555502ed2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/10.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/10.2 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.5/10.2 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m273.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Installed: diffusers, transformers, accelerate, safetensors, streamlit, pillow, cloudflared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generator.py\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "# ---------- Global config ----------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "PIPE = None\n",
        "CURRENT_MODEL_ID = None\n",
        "\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Style presets for prompt engineering\n",
        "STYLE_PRESETS = {\n",
        "    \"None (raw prompt)\": \"\",\n",
        "    \"Photorealistic\": \"highly detailed, 8k, ultra realistic, professional photography, sharp focus, rich lighting\",\n",
        "    \"Digital Art\": \"digital art, concept art, highly detailed, trending on artstation, 4k illustration\",\n",
        "    \"Oil Painting\": \"oil painting, rich brush strokes, dramatic lighting, classic art style\",\n",
        "    \"Cartoon / Anime\": \"anime style, vibrant colors, cel shading, clean lines, highly detailed\",\n",
        "}\n",
        "\n",
        "DEFAULT_NEGATIVE = (\n",
        "    \"low quality, blurry, pixelated, distorted, bad anatomy, extra limbs, \"\n",
        "    \"watermark, text, cropped, worst quality, low resolution\"\n",
        ")\n",
        "\n",
        "\n",
        "def init_model(model_id: str, hf_token: str):\n",
        "    \"\"\"\n",
        "    Lazily load Stable Diffusion model.\n",
        "    Uses GPU if available, otherwise CPU.\n",
        "    \"\"\"\n",
        "    global PIPE, CURRENT_MODEL_ID\n",
        "\n",
        "    if PIPE is not None and CURRENT_MODEL_ID == model_id:\n",
        "        return PIPE\n",
        "\n",
        "    if not hf_token or not hf_token.strip():\n",
        "        raise ValueError(\n",
        "            \"HuggingFace token is required. \"\n",
        "            \"Create one at https://huggingface.co/settings/tokens \"\n",
        "            \"and paste it in the sidebar.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Loading model '{model_id}' on device '{DEVICE}' ...\")\n",
        "\n",
        "    kwargs = {}\n",
        "    if DEVICE == \"cuda\":\n",
        "        kwargs[\"torch_dtype\"] = torch.float16\n",
        "\n",
        "    # NOTE: model requires license acceptance on HuggingFace\n",
        "    PIPE = StableDiffusionPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        use_auth_token=hf_token,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    # Use a faster scheduler\n",
        "    try:\n",
        "        PIPE.scheduler = DPMSolverMultistepScheduler.from_config(PIPE.scheduler.config)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    PIPE = PIPE.to(DEVICE)\n",
        "\n",
        "    if DEVICE == \"cuda\":\n",
        "        PIPE.enable_attention_slicing()\n",
        "\n",
        "    CURRENT_MODEL_ID = model_id\n",
        "    print(\"‚úÖ Model loaded.\")\n",
        "    return PIPE\n",
        "\n",
        "\n",
        "def build_prompt(prompt: str, style_preset: str) -> str:\n",
        "    style_suffix = STYLE_PRESETS.get(style_preset, \"\")\n",
        "    if style_suffix:\n",
        "        return f\"{prompt}, {style_suffix}\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def ensure_run_dir() -> str:\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_dir = os.path.join(OUTPUT_DIR, ts)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "\n",
        "def save_image_with_metadata(img, metadata: dict, run_dir: str,\n",
        "                             base_filename: str, index: int,\n",
        "                             output_formats):\n",
        "    \"\"\"\n",
        "    Save image in PNG/JPEG (as requested) + sidecar JSON with metadata.\n",
        "    Returns dict of {format: filepath}.\n",
        "    \"\"\"\n",
        "    paths = {}\n",
        "    for fmt in output_formats:\n",
        "        fmt = fmt.upper()\n",
        "        ext = fmt.lower()\n",
        "        filename = f\"{base_filename}_{index+1}.{ext}\"\n",
        "        filepath = os.path.join(run_dir, filename)\n",
        "\n",
        "        to_save = img\n",
        "        if fmt == \"JPEG\":\n",
        "            to_save = img.convert(\"RGB\")\n",
        "\n",
        "        to_save.save(filepath, format=fmt)\n",
        "        paths[fmt] = filepath\n",
        "\n",
        "    meta_filename = f\"{base_filename}_{index+1}_meta.json\"\n",
        "    meta_path = os.path.join(run_dir, meta_filename)\n",
        "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    return paths\n",
        "\n",
        "\n",
        "def generate_images(\n",
        "    prompt: str,\n",
        "    negative_prompt: str,\n",
        "    num_images: int,\n",
        "    style_preset: str,\n",
        "    guidance_scale: float,\n",
        "    num_inference_steps: int,\n",
        "    seed: int | None,\n",
        "    hf_token: str,\n",
        "    model_id: str,\n",
        "    base_filename: str,\n",
        "    output_formats,\n",
        "):\n",
        "    \"\"\"\n",
        "    Main API called from Streamlit.\n",
        "    Returns list of dicts: { 'pil_image': img, 'paths': {...}, 'metadata': {...} }\n",
        "    \"\"\"\n",
        "    pipe = init_model(model_id=model_id, hf_token=hf_token)\n",
        "\n",
        "    full_prompt = build_prompt(prompt, style_preset)\n",
        "    neg_prompt = negative_prompt.strip() if negative_prompt.strip() else DEFAULT_NEGATIVE\n",
        "\n",
        "    # Seed / randomness\n",
        "    generator = None\n",
        "    if seed is not None:\n",
        "        generator = torch.Generator(device=DEVICE).manual_seed(int(seed))\n",
        "\n",
        "    run_dir = ensure_run_dir()\n",
        "\n",
        "    # Generate images\n",
        "    result = pipe(\n",
        "        full_prompt,\n",
        "        negative_prompt=neg_prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_images_per_prompt=num_images,\n",
        "        generator=generator,\n",
        "    )\n",
        "\n",
        "    images = result.images\n",
        "\n",
        "    outputs = []\n",
        "    for idx, img in enumerate(images):\n",
        "        meta = {\n",
        "            \"prompt\": prompt,\n",
        "            \"full_prompt\": full_prompt,\n",
        "            \"negative_prompt\": neg_prompt,\n",
        "            \"style_preset\": style_preset,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"num_inference_steps\": num_inference_steps,\n",
        "            \"seed\": seed,\n",
        "            \"model_id\": model_id,\n",
        "            \"index\": idx,\n",
        "            \"device\": DEVICE,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "        }\n",
        "        paths = save_image_with_metadata(\n",
        "            img,\n",
        "            meta,\n",
        "            run_dir=run_dir,\n",
        "            base_filename=base_filename,\n",
        "            index=idx,\n",
        "            output_formats=output_formats,\n",
        "        )\n",
        "        outputs.append({\"pil_image\": img, \"paths\": paths, \"metadata\": meta})\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW6Z2hq8iGq6",
        "outputId": "0d330228-30c9-4e8f-acd9-61866a7f1527"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv_jmTC34FYM",
        "outputId": "8dfa2c12-7825-4bb9-b51f-823a88715689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import io\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "from generator import (\n",
        "    generate_images,\n",
        "    STYLE_PRESETS,\n",
        ")\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI Image Generator (Stable Diffusion)\",\n",
        "    page_icon=\"üé®\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "st.title(\"üé® AI Image Generator (Stable Diffusion)\")\n",
        "st.write(\"Running on **Google Colab** using **open-source Stable Diffusion (Diffusers)**.\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# ---------- Sidebar: configuration ----------\n",
        "st.sidebar.header(\"‚öôÔ∏è Generation Settings\")\n",
        "\n",
        "hf_token = st.sidebar.text_input(\n",
        "    \"HuggingFace Access Token\",\n",
        "    type=\"password\",\n",
        "    help=\"Required for loading Stable Diffusion from HuggingFace Hub.\",\n",
        ")\n",
        "\n",
        "model_id = st.sidebar.text_input(\n",
        "    \"Model ID\",\n",
        "    value=\"runwayml/stable-diffusion-v1-5\",\n",
        "    help=\"Any compatible Stable Diffusion text-to-image model on HuggingFace.\",\n",
        ")\n",
        "\n",
        "num_images = st.sidebar.slider(\n",
        "    \"Number of images per prompt\",\n",
        "    min_value=1,\n",
        "    max_value=4,\n",
        "    value=1,\n",
        ")\n",
        "\n",
        "style_preset = st.sidebar.selectbox(\n",
        "    \"Style preset\",\n",
        "    list(STYLE_PRESETS.keys()),\n",
        "    index=1,  # Photorealistic by default\n",
        ")\n",
        "\n",
        "guidance_scale = st.sidebar.slider(\n",
        "    \"Guidance scale (prompt strength)\",\n",
        "    min_value=3.0,\n",
        "    max_value=15.0,\n",
        "    value=7.5,\n",
        "    step=0.5,\n",
        "    help=\"Higher = follow text more strongly, but may reduce creativity.\",\n",
        ")\n",
        "\n",
        "num_steps = st.sidebar.slider(\n",
        "    \"Diffusion steps\",\n",
        "    min_value=15,\n",
        "    max_value=60,\n",
        "    value=30,\n",
        "    step=5,\n",
        "    help=\"More steps = better quality but slower.\",\n",
        ")\n",
        "\n",
        "seed_value = st.sidebar.number_input(\n",
        "    \"Seed (-1 for random)\",\n",
        "    value=-1,\n",
        "    step=1,\n",
        "    help=\"Use a fixed seed for reproducible images. -1 = random each time.\",\n",
        ")\n",
        "\n",
        "output_formats = st.sidebar.multiselect(\n",
        "    \"Save formats\",\n",
        "    options=[\"PNG\", \"JPEG\"],\n",
        "    default=[\"PNG\", \"JPEG\"],\n",
        ")\n",
        "\n",
        "base_filename = st.sidebar.text_input(\n",
        "    \"Base filename (for saving)\",\n",
        "    value=\"generated_image\",\n",
        ")\n",
        "\n",
        "estimated_time = num_steps * 0.25  # rough, seconds on GPU\n",
        "st.sidebar.caption(\n",
        "    f\"‚è±Ô∏è Estimated time: ~{estimated_time:.1f} sec on GPU, slower on CPU.\"\n",
        ")\n",
        "\n",
        "# ---------- Main area ----------\n",
        "prompt = st.text_area(\n",
        "    \"Enter your image prompt:\",\n",
        "    value=\"a futuristic city at sunset, cinematic view\",\n",
        "    height=100,\n",
        ")\n",
        "\n",
        "negative_prompt = st.text_input(\n",
        "    \"Negative prompt (optional, to avoid unwanted things)\",\n",
        "    value=\"low quality, blurry, distorted, bad anatomy, watermark, text\",\n",
        ")\n",
        "\n",
        "col_btn, _ = st.columns([1, 3])\n",
        "generate_clicked = col_btn.button(\"üöÄ Generate Images\")\n",
        "\n",
        "if generate_clicked:\n",
        "    if not prompt.strip():\n",
        "        st.error(\"Please enter a prompt.\")\n",
        "    elif not hf_token.strip():\n",
        "        st.error(\"Please paste your HuggingFace token in the sidebar.\")\n",
        "    elif not output_formats:\n",
        "        st.error(\"Select at least one output format (PNG/JPEG) in the sidebar.\")\n",
        "    else:\n",
        "        real_seed = None if seed_value < 0 else int(seed_value)\n",
        "\n",
        "        with st.spinner(\"Generating images... this may take a bit on first run (model download).\"):\n",
        "            try:\n",
        "                results = generate_images(\n",
        "                    prompt=prompt,\n",
        "                    negative_prompt=negative_prompt,\n",
        "                    num_images=num_images,\n",
        "                    style_preset=style_preset,\n",
        "                    guidance_scale=guidance_scale,\n",
        "                    num_inference_steps=num_steps,\n",
        "                    seed=real_seed,\n",
        "                    hf_token=hf_token,\n",
        "                    model_id=model_id,\n",
        "                    base_filename=base_filename,\n",
        "                    output_formats=output_formats,\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Generation failed: {e}\")\n",
        "            else:\n",
        "                st.success(f\"Generated {len(results)} image(s). Scroll down to view & download.\")\n",
        "\n",
        "                for idx, out in enumerate(results):\n",
        "                    img = out[\"pil_image\"]\n",
        "                    meta = out[\"metadata\"]\n",
        "\n",
        "                    st.markdown(f\"### üñºÔ∏è Image {idx+1}\")\n",
        "                    st.image(img, use_column_width=True)\n",
        "\n",
        "                    # Download buttons for each requested format\n",
        "                    for fmt in output_formats:\n",
        "                        buf = io.BytesIO()\n",
        "                        save_fmt = fmt.upper()\n",
        "                        img_to_save = img\n",
        "                        if save_fmt == \"JPEG\":\n",
        "                            img_to_save = img.convert(\"RGB\")\n",
        "\n",
        "                        img_to_save.save(buf, format=save_fmt)\n",
        "                        st.download_button(\n",
        "                            label=f\"Download Image {idx+1} as {save_fmt}\",\n",
        "                            data=buf.getvalue(),\n",
        "                            file_name=f\"{base_filename}_{idx+1}.{save_fmt.lower()}\",\n",
        "                            mime=f\"image/{save_fmt.lower()}\",\n",
        "                            key=f\"download_{idx}_{save_fmt}\",\n",
        "                        )\n",
        "\n",
        "                    with st.expander(f\"Metadata for image {idx+1}\"):\n",
        "                        st.json(meta)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL 4: Start Streamlit server ====\n",
        "\n",
        "# Kill any previous Streamlit processes\n",
        "!pkill -f streamlit || echo \"No previous Streamlit process.\"\n",
        "\n",
        "# Start new app\n",
        "!streamlit run app.py --server.address 0.0.0.0 --server.port 8501 > logs.txt 2>&1 &\n",
        "\n",
        "print(\"‚úÖ Streamlit server started on port 8501. If something looks wrong, check logs in the next cell.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_7nt_QFqLqs",
        "outputId": "5b870390-c88a-41a6-eeea-bc077c30468e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "‚úÖ Streamlit server started on port 8501. If something looks wrong, check logs in the next cell.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL 5: View first 150 lines of logs if debugging ====\n",
        "!sed -n '1,150p' logs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yl14LU9xwGF",
        "outputId": "dcb47cfb-0a7e-4aa5-c049-e09ea559aaf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CELL 6: Create public URL with Cloudflare ====\n",
        "# Run this cell and WAIT until you see a \"trycloudflare.com\" URL.\n",
        "\n",
        "!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1MqxC5Kx6fk",
        "outputId": "f50522dc-79c1-4229-8365-ef62b10c4685"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[90m2025-11-28T09:46:14Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-28T09:46:14Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m |  https://ref-priest-rome-devices.trycloudflare.com                                         |\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: a8bfec45-661c-4e75-aa56-849e49ae1c8d\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-28T09:46:16Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-28T09:46:17Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "2025/11/28 09:46:17 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-28T09:46:17Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m85af53b9-fbf7-47ef-9299-e92046ce25cf \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDHYx33a2XI1",
        "outputId": "9dbc6252-6999-4fbc-b012-d557e6f90592"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 28 09:44:29 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}